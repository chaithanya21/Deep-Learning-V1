<h1>Cross-Entropy:Log-Loss Error: The Measure of Difference Between the Probability Distributions</h1>

Cross Entropy is also called as the Error Function/Loss/Cost Function for the Classification Problems.

<h3>The Error Function should be always continuous in order to minimize the error(through backpropagation/Gradient Descent) and also it shoud be differentiable</h3>

<h3>Maximum Likelihood Estimation is Inversly Proportional to Cross-Entropy,hence a Model with higher likelihood Estimation(higher Probabilistic Value) has low Cross-Entropy</h3>

Hence a Maximizing the Probabilities is similar to that of Minimizing the Error Function.

[To Know More about the Cross Entropy: Visit this Link by MachineLearning Mastery-A GENTLE INTRO TO CROSS-ENTROPY)](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
